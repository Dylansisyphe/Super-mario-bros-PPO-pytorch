{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym) (6.6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nes-py in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (8.2.1)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py) (4.65.0)\n",
      "Requirement already satisfied: gym>=0.17.2 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py) (1.24.3)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py) (1.5.7)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym>=0.17.2->nes-py) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym>=0.17.2->nes-py) (6.6.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym>=0.17.2->nes-py) (0.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gym-super-mario-bros in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (7.3.2)\n",
      "Requirement already satisfied: nes-py>=8.1.2 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym-super-mario-bros) (8.2.1)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.7)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.24.3)\n",
      "Requirement already satisfied: gym>=0.17.2 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.65.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym\n",
    "%pip install nes-py\n",
    "%pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maqisen/miniconda3/envs/ppo/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as _mp\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym.spaces import Box\n",
    "from gym import Wrapper\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_env(world, stage, actions, output_path=None):\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(world, stage))\n",
    "\n",
    "    env = JoypadSpace(env, actions)\n",
    "    env = CustomReward(env, world, stage, monitor=None)\n",
    "    env = CustomSkipFrame(env)\n",
    "    return env\n",
    "\n",
    "class CustomReward(Wrapper):\n",
    "    def __init__(self, env=None, world=None, stage=None, monitor=None):\n",
    "        super(CustomReward, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 84, 84))\n",
    "        self.curr_score = 0\n",
    "        self.current_x = 40\n",
    "        self.world = world\n",
    "        self.stage = stage\n",
    "        if monitor:\n",
    "            self.monitor = monitor\n",
    "        else:\n",
    "            self.monitor = None\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if self.monitor:\n",
    "            self.monitor.record(state)\n",
    "        state = process_frame(state)\n",
    "        reward += (info[\"score\"] - self.curr_score) / 40.\n",
    "        self.curr_score = info[\"score\"]\n",
    "        if done:\n",
    "            if info[\"flag_get\"]:\n",
    "                reward += 50\n",
    "            else:\n",
    "                reward -= 50\n",
    "        if self.world == 7 and self.stage == 4:\n",
    "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
    "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
    "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
    "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
    "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
    "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
    "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191) or info[\"x_pos\"] < self.current_x - 500:\n",
    "                reward -= 50\n",
    "                done = True\n",
    "        if self.world == 4 and self.stage == 4:\n",
    "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
    "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
    "                reward = -50\n",
    "                done = True\n",
    "\n",
    "        self.current_x = info[\"x_pos\"]\n",
    "        return state, reward / 10., done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_score = 0\n",
    "        self.current_x = 40\n",
    "        return process_frame(self.env.reset())\n",
    "    \n",
    "class MultipleEnvironments:\n",
    "    def __init__(self, world, stage, action_type, num_envs, output_path=None):\n",
    "        self.agent_conns, self.env_conns = zip(*[mp.Pipe() for _ in range(num_envs)])\n",
    "        if action_type == \"right_only\":\n",
    "            actions = RIGHT_ONLY\n",
    "        elif action_type == \"simple\":\n",
    "            actions = SIMPLE_MOVEMENT\n",
    "        else:\n",
    "            actions = COMPLEX_MOVEMENT\n",
    "        self.envs = [create_train_env(world, stage, actions, output_path=output_path) for _ in range(num_envs)]\n",
    "        self.num_states = self.envs[0].observation_space.shape[0]\n",
    "        self.num_actions = len(actions)\n",
    "        for index in range(num_envs):\n",
    "            process = mp.Process(target=self.run, args=(index,))\n",
    "            process.start()\n",
    "            self.env_conns[index].close()\n",
    "\n",
    "    def run(self, index):\n",
    "        self.agent_conns[index].close()\n",
    "        while True:\n",
    "            request, action = self.env_conns[index].recv()\n",
    "            if request == \"step\":\n",
    "                self.env_conns[index].send(self.envs[index].step(action.item()))\n",
    "            elif request == \"reset\":\n",
    "                self.env_conns[index].send(self.envs[index].reset())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "                \n",
    "def process_frame(frame):\n",
    "    if frame is not None:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255.\n",
    "        return frame\n",
    "    else:\n",
    "        return np.zeros((1, 84, 84))\n",
    "    \n",
    "class CustomSkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super(CustomSkipFrame, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(skip, 84, 84))\n",
    "        self.skip = skip\n",
    "        self.states = np.zeros((skip, 84, 84), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        last_states = []\n",
    "        for i in range(self.skip):\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if i >= self.skip / 2:\n",
    "                last_states.append(state)\n",
    "            if done:\n",
    "                self.reset()\n",
    "                return self.states[None, :, :, :].astype(np.float32), total_reward, done, info\n",
    "        max_state = np.max(np.concatenate(last_states, 0), 0)\n",
    "        self.states[:-1] = self.states[1:]\n",
    "        self.states[-1] = max_state\n",
    "        return self.states[None, :, :, :].astype(np.float32), total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.states = np.concatenate([state for _ in range(self.skip)], 0)\n",
    "        return self.states[None, :, :, :].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.linear = nn.Linear(32 * 6 * 6, 512)\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_actions)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, nn.init.calculate_gain('relu'))\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.linear(x.view(x.size(0), -1))\n",
    "        return self.actor_linear(x), self.critic_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(opt, global_model, num_states, num_actions,curr_episode):\n",
    "    print('start evalution !')\n",
    "    torch.manual_seed(123)\n",
    "    if opt['action_type'] == \"right\":\n",
    "        actions = RIGHT_ONLY\n",
    "    elif opt['action_type'] == \"simple\":\n",
    "        actions = SIMPLE_MOVEMENT\n",
    "    else:\n",
    "        actions = COMPLEX_MOVEMENT\n",
    "    env = create_train_env(opt['world'], opt['stage'], actions)\n",
    "    local_model = Net(num_states, num_actions)\n",
    "    if torch.cuda.is_available():\n",
    "        local_model.cuda()\n",
    "    local_model.eval()\n",
    "    state = torch.from_numpy(env.reset())\n",
    "    if torch.cuda.is_available():\n",
    "        state = state.cuda()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    \n",
    "    done=False\n",
    "    local_model.load_state_dict(global_model.state_dict()) #加载网络参数\\\n",
    "\n",
    "    while not done:\n",
    "        if torch.cuda.is_available():\n",
    "            state = state.cuda()\n",
    "        logits, value = local_model(state)\n",
    "        policy = F.softmax(logits, dim=1)\n",
    "        action = torch.argmax(policy).item()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = torch.from_numpy(state)\n",
    "        \n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        if info[\"flag_get\"]:\n",
    "            print(\"flag getted in episode:{}!\".format(curr_episode))\n",
    "            torch.save(local_model.state_dict(),\n",
    "                       \"{}/ppo_super_mario_bros_{}_{}_{}\".format(opt['saved_path'], opt['world'], opt['stage'],curr_episode))\n",
    "            opt.update({'episode':curr_episode})\n",
    "            env.close()\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "    \n",
    "def train(opt):\n",
    "    #判断cuda是否可用\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "    if os.path.isdir(opt['log_path']):\n",
    "        shutil.rmtree(opt['log_path'])\n",
    "\n",
    "    os.makedirs(opt['log_path'])\n",
    "    if not os.path.isdir(opt['saved_path']):\n",
    "        os.makedirs(opt['saved_path'])\n",
    "    mp = _mp.get_context(\"spawn\")\n",
    "    #创建环境\n",
    "    envs = MultipleEnvironments(opt['world'], opt['stage'], opt['action_type'], opt['num_processes'])\n",
    "    #创建模型\n",
    "    model = Net(envs.num_states, envs.num_actions)\n",
    "    if opt['pretrain_model']:\n",
    "        print('加载预训练模型')\n",
    "        if not os.path.exists(\"ppo_super_mario_bros_1_1_0\"):\n",
    "            mox.file.copy_parallel(\n",
    "                \"obs://modelarts-labs-bj4/course/modelarts/zjc_team/reinforcement_learning/ppo_mario/ppo_super_mario_bros_1_1_0\",\n",
    "                \"ppo_super_mario_bros_1_1_0\")\n",
    "        if torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(\"ppo_super_mario_bros_1_1_0\"))\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(\"ppo_super_mario_bros_1_1_0\"),\n",
    "                                             map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "         model.cuda()\n",
    "    model.share_memory()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=opt['lr'])\n",
    "    #环境重置\n",
    "    [agent_conn.send((\"reset\", None)) for agent_conn in envs.agent_conns]\n",
    "    #接收当前状态\n",
    "    curr_states = [agent_conn.recv() for agent_conn in envs.agent_conns]\n",
    "    curr_states = torch.from_numpy(np.concatenate(curr_states, 0))\n",
    "    if torch.cuda.is_available():\n",
    "        curr_states = curr_states.cuda()\n",
    "    curr_episode = 0\n",
    "    #在最大回合内训练\n",
    "    while curr_episode<opt['max_episode']:\n",
    "        if curr_episode % opt['save_interval'] == 0 and curr_episode > 0:\n",
    "            torch.save(model.state_dict(),\n",
    "                       \"{}/ppo_super_mario_bros_{}_{}_{}\".format(opt['saved_path'], opt['world'], opt['stage'], curr_episode))\n",
    "        curr_episode += 1\n",
    "        old_log_policies = []\n",
    "        actions = []\n",
    "        values = []\n",
    "        states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        #一回合内最大步数\n",
    "        for _ in range(opt['num_local_steps']):\n",
    "            states.append(curr_states)\n",
    "            logits, value = model(curr_states)\n",
    "            values.append(value.squeeze())\n",
    "            policy = F.softmax(logits, dim=1)\n",
    "            old_m = Categorical(policy)\n",
    "            action = old_m.sample()\n",
    "            actions.append(action)\n",
    "            old_log_policy = old_m.log_prob(action)\n",
    "            old_log_policies.append(old_log_policy)\n",
    "            #执行action\n",
    "            if torch.cuda.is_available():\n",
    "                [agent_conn.send((\"step\", act)) for agent_conn, act in zip(envs.agent_conns, action.cpu())]\n",
    "            else:\n",
    "                [agent_conn.send((\"step\", act)) for agent_conn, act in zip(envs.agent_conns, action)]\n",
    "            state, reward, done, info = zip(*[agent_conn.recv() for agent_conn in envs.agent_conns])\n",
    "            state = torch.from_numpy(np.concatenate(state, 0))\n",
    "            if torch.cuda.is_available():\n",
    "                state = state.cuda()\n",
    "                reward = torch.cuda.FloatTensor(reward)\n",
    "                done = torch.cuda.FloatTensor(done)\n",
    "            else:\n",
    "                reward = torch.FloatTensor(reward)\n",
    "                done = torch.FloatTensor(done)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            curr_states = state\n",
    "\n",
    "        _, next_value, = model(curr_states)\n",
    "        next_value = next_value.squeeze()\n",
    "        old_log_policies = torch.cat(old_log_policies).detach()\n",
    "        actions = torch.cat(actions)\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        gae = 0\n",
    "        R = []\n",
    "        #gae计算\n",
    "        for value, reward, done in list(zip(values, rewards, dones))[::-1]:\n",
    "            gae = gae * opt['gamma'] * opt['tau']\n",
    "            gae = gae + reward + opt['gamma'] * next_value.detach() * (1 - done) - value.detach()\n",
    "            next_value = value\n",
    "            R.append(gae + value)\n",
    "        R = R[::-1]\n",
    "        R = torch.cat(R).detach()\n",
    "        advantages = R - values\n",
    "        #策略更新\n",
    "        for i in range(opt['num_epochs']):\n",
    "            indice = torch.randperm(opt['num_local_steps'] * opt['num_processes'])\n",
    "            for j in range(opt['batch_size']):\n",
    "                batch_indices = indice[\n",
    "                                int(j * (opt['num_local_steps'] * opt['num_processes'] / opt['batch_size'])): int((j + 1) * (\n",
    "                                        opt['num_local_steps'] * opt['num_processes'] / opt['batch_size']))]\n",
    "                logits, value = model(states[batch_indices])\n",
    "                new_policy = F.softmax(logits, dim=1)\n",
    "                new_m = Categorical(new_policy)\n",
    "                new_log_policy = new_m.log_prob(actions[batch_indices])\n",
    "                ratio = torch.exp(new_log_policy - old_log_policies[batch_indices])\n",
    "                actor_loss = -torch.mean(torch.min(ratio * advantages[batch_indices],\n",
    "                                                   torch.clamp(ratio, 1.0 - opt['epsilon'], 1.0 + opt['epsilon']) *\n",
    "                                                   advantages[\n",
    "                                                       batch_indices]))\n",
    "                critic_loss = F.smooth_l1_loss(R[batch_indices], value.squeeze())\n",
    "                entropy_loss = torch.mean(new_m.entropy())\n",
    "                #损失函数包含三个部分：actor损失，critic损失，和动作entropy损失\n",
    "                total_loss = actor_loss + critic_loss - opt['beta'] * entropy_loss\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "        print(\"Episode: {}. Total loss: {}\".format(curr_episode, total_loss))\n",
    "        \n",
    "        finish=False\n",
    "        for i in range(opt[\"num_processes\"]):\n",
    "            if info[i][\"flag_get\"]:\n",
    "                finish=evaluation(opt, model,envs.num_states, envs.num_actions,curr_episode)\n",
    "                if finish:\n",
    "                    break\n",
    "        if finish:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "705be87f2920c7fc7a5b08bef63ce09ce44f944347e1fc7e277d28f3e6430b82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
